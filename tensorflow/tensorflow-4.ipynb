{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import tensorflow as tf"
    },
    {
      "cell_type": "markdown",
      "source": "## 关于Optimizer\nTensorFlow会自动求导，然后更新参数，使用一行代码tf.train.GradientDescentOptimizer(learning_rate\u003d1e-3).minimize(loss)，下面我们将其细分开来，讲一讲每一步。\n### 自动梯度\n首先优化函数的定义就是前面一部分opt \u003d tf.train.GradientDescentOptimizer(learning_rate)，定义好优化函数之后，可以通过grads_and_vars \u003d opt.compute_gradients(loss, \u003clist of variables\u003e)来计算loss对于一个变量列表里面每一个变量的梯度，得到的grads_and_vars是一个list of tuples，list中的每个tuple都是由(gradient, variable)构成的，我们可以通过get_grads_and_vars \u003d [(gv[0], gv[1]) for gv in grads_and_vars]将其分别取出来，然后通过opt.apply_gradients(get_grads_and_vars)来更新里面的参数，下面我们举一个小例子。",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "grads and variables\nx: grad 13.0, value 5.0\ny: grad 5.0, value 3.0\nBefore optimization\nx: 5.0, y: 3.0\nAfter optimization using learning rate 0.1\nx: 3.700, y: 2.500\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nx \u003d tf.Variable(5, dtype\u003dtf.float32)\ny \u003d tf.Variable(3, dtype\u003dtf.float32)\n\nz \u003d x**2 + x * y + 3\n\nsess \u003d tf.Session()\n# initialize variable\nsess.run(tf.global_variables_initializer())\n\n# define optimizer\noptimizer \u003d tf.train.GradientDescentOptimizer(0.1)\n\n# compute gradient z w.r.t x and y\ngrads_and_vars \u003d optimizer.compute_gradients(z, [x, y])\n\n# fetch the variable\nget_grads_and_vars \u003d [(gv[0], gv[1]) for gv in grads_and_vars]\n\n# dz/dx \u003d 2*x + y\u003d 13\n# dz/dy \u003d x \u003d 5\nprint(\u0027grads and variables\u0027)\nprint(\u0027x: grad {}, value {}\u0027.format(\nsess.run(get_grads_and_vars[0][0]), sess.run(get_grads_and_vars[0][1])))\n\nprint(\u0027y: grad {}, value {}\u0027.format(\nsess.run(get_grads_and_vars[1][0]), sess.run(get_grads_and_vars[1][1])))\n\nprint(\u0027Before optimization\u0027)\nprint(\u0027x: {}, y: {}\u0027.format(sess.run(x), sess.run(y)))\n\n# optimize parameters\nopt \u003d optimizer.apply_gradients(get_grads_and_vars)\n# x \u003d x - 0.1 * dz/dx \u003d 5 - 0.1 * 13 \u003d 3.7\n# y \u003d y - 0.1 * dz/dy \u003d 3 - 0.1 * 5 \u003d 2.5\nprint(\u0027After optimization using learning rate 0.1\u0027)\nsess.run(opt)\nprint(\u0027x: {:.3f}, y: {:.3f}\u0027.format(sess.run(x), sess.run(y)))\nsess.close()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "在实际中，我们当然不用手动更新参数，optimizer类可以帮我们自动更新，另外还有一个函数也能够计算梯度。\n```\ntf.gradients(ys, xs, grad_ys\u003dNone, name\u003d\u0027gradients\u0027, colocate_gradients_with_ops\u003dFalse, gate_gradients\u003dFalse, aggregation_method\u003dNone)\n```\n这个函数会返回list，list的长度就是xs的长度，list中每个元素都是![公式](https://www.zhihu.com/equation?tex\u003dsum_%7Bys%7D%28dys%2Fdx%29)。\n- 实际运用: 这个方法对于只训练部分网络非常有用，我们能够使用上面的函数只对网络中一部分参数求梯度，然后对他们进行梯度的更新。\n\n### 优化函数类型\n随机梯度下降(GradientDescentOptimizer)仅仅只是tensorflow中一个小的更新方法，下面是tensorflow目前支持的更新方法的总结\n```\ntf.train.GradientDescentOptimizer\ntf.train.AdadeltaOptimizer\ntf.train.AdagradOptimizer\ntf.train.AdagradDAOptimizer\ntf.train.MomentumOptimizer\ntf.train.AdamOptimizer\ntf.train.FtrlOptimizer\ntf.train.ProximalGradientDescentOptimizer\ntf.train.ProximalAdagradOptimizer\ntf.train.RMSPropOptimizer\n```\n这个[博客](https://link.zhihu.com/?target\u003dhttp%3A//sebastianruder.com/optimizing-gradient-descent/)对上面的方法都做了介绍，感兴趣的同学可以去看看，另外cs231n和coursera的神经网络课程也对各种优化算法做了介绍。",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## TensorFlow 中的Logistic Regression\n我们使用简单的logistic regression来解决分类问题，使用MNIST手写字体，我们的模型公式如下\n![](https://www.zhihu.com/equation?tex\u003dlogits+%3D+X+%2A+w+%2B+b+%5C%5C+Y_%7Bpredicted%7D+%3D+softmax%28logits%29%5C%5C+loss+%3D+CrossEntropy%28Y%2C+Y_%7Bpredicted%7D%29)\n\n### TensorFlow实现\nTF Learn中内置了一个脚本可以读取MNIST数据集",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From \u003cipython-input-3-e0bd8add32f4\u003e:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From D:\\PythonIDE\\anaconda3\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\n",
            "WARNING:tensorflow:From D:\\PythonIDE\\anaconda3\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.\u003clocals\u003e.wrap.\u003clocals\u003e.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nWARNING:tensorflow:From D:\\PythonIDE\\anaconda3\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
            "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nWARNING:tensorflow:From D:\\PythonIDE\\anaconda3\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
            "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From D:\\PythonIDE\\anaconda3\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From D:\\PythonIDE\\anaconda3\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from tensorflow.examples.tutorials.mnist import input_data\nmnist \u003d input_data.read_data_sets(\u0027./data/mnist\u0027, one_hot\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "接着定义占位符(placeholder)和权重参数",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": "x \u003d tf.placeholder(tf.float32, shape\u003d[None, 784], name\u003d\u0027image\u0027)\ny \u003d tf.placeholder(tf.int32, shape\u003d[None, 10], name\u003d\u0027label\u0027)\n\nw \u003d tf.get_variable(\u0027weight\u0027, shape\u003d[784, 10], initializer\u003dtf.truncated_normal_initializer())\nb \u003d tf.get_variable(\u0027bias\u0027, shape\u003d[10], initializer\u003dtf.zeros_initializer())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "输入数据的shape\u003d[None, 784]表示第一维接受任何长度的输入，第二维等于784是因为28x28\u003d784。权重w使用均值为0,方差为1的正态分布，偏置b初始化为0。\n\n然后定义预测结果、loss和优化函数",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From \u003cipython-input-5-5773d3f4cc3c\u003e:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "learning_rate \u003d 0.1\nlogits \u003d tf.matmul(x, w) + b\nentropy \u003d tf.nn.softmax_cross_entropy_with_logits(labels\u003dy, logits\u003dlogits)\nloss \u003d tf.reduce_mean(entropy, axis\u003d0)\noptimizer \u003d tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "使用tf.matmul做矩阵乘法，然后使用分类问题的loss函数交叉熵，最后将一个batch中的loss求均值，对其使用随机梯度下降法\n\n因为数据集中有测试集，所以可以在测试集上验证其准确率",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "preds \u003d tf.nn.softmax(logits)\ncorrect_preds \u003d tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\naccuracy \u003d tf.reduce_sum(tf.cast(correct_preds, tf.float32), axis\u003d0)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "首先对输出结果进行softmax得到概率分布，然后使用tf.argmax得到预测的label，使用tf.equal得到预测的label和实际的label相同的个数，这是一个长为batch的0-1向量，然后使用tf.reduce_sum得到正确的总数。\n\n最后在session中运算，这个过程就不再赘述。",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 结果与可视化\n最后可以得到训练集的loss的验证集准确率如下",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Average loss epoch 0: 2.6075366517324827\n",
            "Average loss epoch 1: 1.038326608273255\n",
            "Average loss epoch 2: 0.8234267927410998\n",
            "Average loss epoch 3: 0.7197303167589895\n",
            "Average loss epoch 4: 0.659674979277424\n",
            "Average loss epoch 5: 0.6105360811129039\n",
            "Average loss epoch 6: 0.5785137918584552\n",
            "Average loss epoch 7: 0.5510510732799699\n",
            "Average loss epoch 8: 0.5296188234101921\n",
            "Average loss epoch 9: 0.5095977137138793\nTotal time: 6.411830186843872 seconds\nOptimization Finished!\n",
            "Accuracy 0.8773\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import time\nbatch_size \u003d 128\nn_epochs \u003d 10\n\nwith tf.Session() as sess:\n    writer \u003d tf.summary.FileWriter(\u0027./logistic_log\u0027, sess.graph)\n    start_time \u003d time.time()\n    sess.run(tf.global_variables_initializer())\n    n_batches \u003d int(mnist.train.num_examples / batch_size)\n    for i in range(n_epochs):  # train the model n_epochs times\n        total_loss \u003d 0\n        for _ in range(n_batches):\n            X_batch, Y_batch \u003d mnist.train.next_batch(batch_size)\n            _, loss_batch \u003d sess.run(\n                [optimizer, loss], feed_dict\u003d{x: X_batch,\n                                              y: Y_batch})\n            total_loss +\u003d loss_batch\n        print(\u0027Average loss epoch {0}: {1}\u0027.format(i, total_loss / n_batches))\n\n    print(\u0027Total time: {0} seconds\u0027.format(time.time() - start_time))\n\n    print(\u0027Optimization Finished!\u0027)  # should be around 0.35 after 25 epochs\n\n    # test the model\n    n_batches \u003d int(mnist.test.num_examples / batch_size)\n    total_correct_preds \u003d 0\n\n    for i in range(n_batches):\n        X_batch, Y_batch \u003d mnist.test.next_batch(batch_size)\n        accuracy_batch \u003d sess.run(accuracy, feed_dict\u003d{x: X_batch, y: Y_batch})\n        total_correct_preds +\u003d accuracy_batch\n\n    print(\u0027Accuracy {0}\u0027.format(total_correct_preds / mnist.test.num_examples))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "pycharm-b977b55f",
      "language": "python",
      "display_name": "PyCharm (OpenCVLearn)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}